---
layout: post
category: 技术
title: Scrapy框架
tags: 爬虫
---

###Scrapy
====

####安装软件
- 安装Scrapy
		
		sudo pip install Scrapy
- 安装openssl

		sudo pip install openssl
		
-----

-  在抓取之前，你需要新建一个Scrapy工程。进入一个你想用来保存代码的目录，然后执行：
		
		scrapy startproject tutorial	
- 当前目录下创建一个新目录tutorial，它的结构如下：

	- scrapy.cfg: 项目配置文件
	- tutorial/: 项目python模块,代码将从这里导入
	- tutorial/items.py: 项目items文件
	- tutorial/pipelines.py: 项目管道文件
	- tutorial/settings.py: 项目配置文件
	- tutorial/spiders: 放置spider的目录
- `Items`是将要装载抓取的数据的容器，它工作方式像python里面的字典。
	(它通过创建一个scrapy.item.Item类来声明)
- 要建立一个Spider(爬虫)，你必须为`scrapy.spider.BaseSpider`创建一个子类，并确定三个主要的、强制的属性：

	- **name：**爬虫的识别名，它必须是唯一的，在不同的爬虫中你必须定义不同的名字.
	- **start_urls：**爬虫开始爬的一个URL列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些URLS开始。其他子URL将会从这些起始URL中继承性生成。
	- **parse()：**爬虫的方法，调用时候传入从**每一个URL传回的Response对象作为参数**，response将会是parse方法的**唯一的一个参数**,
这个方法负责解析返回的数据、匹配抓取的数据(解析为item)并跟踪更多的URL。

-  scrapy crawl dmoz（crawl dmoz 命令从dmoz.org域启动爬虫）
-  **从网站中提取数据**，Scrapy 使用一种叫做`XPath和 CSS 表达式机制`，它基于 `XPath`表达式。
-  XPath表达式的例子和他们的含义:
		
		- /html/head/title: 选择HTML文档<head>元素下面的<title> 标签。
		- /html/head/title/text(): 选择前面提到的<title> 元素下面的文本内容
		- //td: 选择所有 <td> 元素
		- //div[@class="mine"]: 选择所有包含 class="mine" 属性的div 标签元素

- 为方便使用XPaths，Scrapy提供`XPathSelector`类,`HtmlXPathSelector (HTML数据解析)`和`XmlXPathSelector(XML数据解析)`。
- Selector对象展示了文档的节点结构。因此，第一个实例化的selector必与根节点或者是整个目录有关
- Selectors 有四种方法:

	- xpath()：传入xpath表达式，返回该表达式所对应的所有节点的selector list**列表**
	- css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list**列表**.
	- extract()：序列化该节点为unicode**字符串**并返回list
	- re()： 根据传入的正则表达式对数据进行提取，返回unicode**字符串list列表**
	
- 例子：

		sel.xpath('//ul/li/a/@href').extract()  
		列表的方式获得网站的链
	
- 实例(将要使用内置的 Scrapy shell )在终端中输入
		
			scrapy shell http://www.dmoz.org/Computers/Programming/Languages/Python/Books/
			Shell载入后，你将获得回应，这些内容被存储在本地变量 response 中
		所以如果你输入response.body 你将会看到response的body部分，或者输入response.headers 来查看它的 header部分。 
		更为重要的是，当输入 response.selector 时， 您将获取到一个可以用于查询返回数据的selector(选择器)， 以及映射到 response.selector.xpath() 、 response.selector.css() 的 快捷方法(shortcut): response.xpath() 和 response.css() 
		同时，shell根据response提前初始化了变量 sel 。该selector根据response的类型自动选择最合适的分析规则(XML vs HTML)。
- 保存爬取到的数据
	
		scrapy crawl dmoz -o items.json
		该命令将采用 JSON 格式对爬取的数据进行序列化，生成 items.json 文件。
		












		